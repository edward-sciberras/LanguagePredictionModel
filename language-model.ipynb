{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk, re, string, collections, json\n",
    "from nltk.util import *\n",
    "from nltk.lm import *\n",
    "import xml.etree.cElementTree as et\n",
    "from nltk.collocations import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXml(folderName, wordList):\n",
    "    # getting current working directory and adding folder containing dataset\n",
    "    cwd = os.getcwd()\n",
    "    default_cwd = cwd + folderName\n",
    "\n",
    "    # getting list of file names in folder containing dataset\n",
    "    file_names = os.listdir(os.getcwd() + folderName[:-1]) # can not use the last / here\n",
    "    \n",
    "    # looping every filename and checking that it ends in .xml\n",
    "    for file in file_names:\n",
    "        if file.endswith('.xml'):\n",
    "            cwd = default_cwd + file # adding filename to file path\n",
    "            tree = et.parse(cwd)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # looping through every sentence tag in files and adding all words and punctuation to list\n",
    "            for item in root.iter('s'):\n",
    "                wordList.append(startingTag)\n",
    "                for word in item:\n",
    "                    if(isinstance(word.text, str)):\n",
    "                        # checked that string is of type string and removing extra whitespace\n",
    "                        wordList.append(word.text.lower().strip())\n",
    "                wordList.append(closingTag)\n",
    "                \n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [] \n",
    "startingTag = \"<s>\"\n",
    "closingTag = \"</s>\"\n",
    "unkString = '<UNK>'\n",
    "\n",
    "words = parseXml('\\\\training_set\\\\', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get different ngrams by inputting list and length\n",
    "def getNgrams(corpus, length):\n",
    "    ngram = ngrams(corpus, length) \n",
    "    return collections.Counter(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get word/s frequencies into a dictionary \n",
    "def getVanillaFreqs(ngram):\n",
    "    freqDict = {}\n",
    "    \n",
    "    # converting ngrams to dictionaries for easier use\n",
    "    ngramDict = dict(ngram)\n",
    "    \n",
    "    # creating lists of keys and values\n",
    "    ngramKeyList = list(ngramDict.keys()) \n",
    "    ngramValueList = list(ngramDict.values()) \n",
    "    \n",
    "    for i in range(len(ngramDict)):\n",
    "        currKey = ngramKeyList[i] # getting word/s \n",
    "        currValue = ngramValueList[i] # getting number of appearences\n",
    "\n",
    "        # calculating percentage of frequency\n",
    "        freq = (currValue / len(words))\n",
    "\n",
    "        freqDict.update({currKey:freq})\n",
    "        \n",
    "    return freqDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanillaUnigram = getNgrams(words, 1)\n",
    "vanillaBigram = getNgrams(words, 2)\n",
    "vanillaTrigram = getNgrams(words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanillaUnigramFreqs = getVanillaFreqs(vanillaUnigram)\n",
    "vanillaBigramFreqs = getVanillaFreqs(vanillaBigram)\n",
    "vanillaTrigramFreqs = getVanillaFreqs(vanillaTrigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to smooth the counts of words \n",
    "def laplaceSmoothing(ngram):\n",
    "    # converting ngrams to dictionaries for easier use\n",
    "    ngramDict = dict(ngram)\n",
    "    \n",
    "    # creating lists of keys and values\n",
    "    ngramKeyList = list(ngramDict.keys()) \n",
    "    ngramValueList = list(ngramDict.values()) \n",
    "    \n",
    "    for i in range(len(ngramDict)):\n",
    "        currKey = ngramKeyList[i] # getting word/s \n",
    "        currValue = ngramValueList[i] # getting number of appearences\n",
    "        \n",
    "        tempItem = {(currKey): currValue + 1}\n",
    "        ngramDict.update(tempItem)\n",
    "        \n",
    "    return ngramDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get freq from dict of word/s counts\n",
    "def getLaplaceFreqs(laplaceNgram):\n",
    "    freqDict = {}\n",
    "    \n",
    "    # creating lists of keys and values\n",
    "    ngramKeyList = list(laplaceNgram.keys()) \n",
    "    ngramValueList = list(laplaceNgram.values()) \n",
    "\n",
    "    for i in range(len(laplaceNgram)):\n",
    "        currKey = ngramKeyList[i] # getting word/s \n",
    "        currValue = ngramValueList[i] # getting number of appearences\n",
    "\n",
    "        # calculating percentage of frequency\n",
    "        freq = (currValue / (len(words) + len(laplaceNgram)))\n",
    "\n",
    "        freqDict.update({currKey:freq})\n",
    "        \n",
    "    return freqDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting laplace smoothened counts\n",
    "laplaceUnigram = laplaceSmoothing(vanillaUnigram)\n",
    "laplaceBigram = laplaceSmoothing(vanillaBigram)\n",
    "laplaceTrigram = laplaceSmoothing(vanillaTrigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating laplace percentages\n",
    "laplaceUnigramFreqs = getLaplaceFreqs(laplaceUnigram)\n",
    "laplaceBigramFreqs = getLaplaceFreqs(laplaceBigram)\n",
    "laplaceTrigramFreqs = getLaplaceFreqs(laplaceTrigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to insert <UNK> tokens in counts\n",
    "def unkTokenInsertion(ngram):\n",
    "    unkTreshold = 2\n",
    "    toDeleteKeys = []\n",
    "    # converting ngrams to dictionaries for easier use\n",
    "    ngramDict = dict(ngram)\n",
    "    \n",
    "    # creating lists of keys and values\n",
    "    ngramKeyList = list(ngramDict.keys()) \n",
    "    ngramValueList = list(ngramDict.values()) \n",
    "    \n",
    "    for i in range(len(ngramDict)):\n",
    "        currKey = ngramKeyList[i] # getting word/s \n",
    "        currValue = ngramValueList[i] # getting number of appearences\n",
    "        \n",
    "        if currValue <= unkTreshold:\n",
    "            # create UNK dictionary entry if not present\n",
    "            if ngramDict.get(unkString) == None:\n",
    "                ngramDict.update({unkString:1})\n",
    "            else:\n",
    "                unkValue = ngramDict.get(unkString) + 1\n",
    "                ngramDict.update({unkString:unkValue})\n",
    "                # adding key to list of keys to delete\n",
    "                toDeleteKeys.append(currKey)\n",
    "        else:\n",
    "            tempItem = {(currKey): currValue + 1}\n",
    "            ngramDict.update(tempItem)\n",
    "    \n",
    "    # deleting items after to avoid out of bounds exception\n",
    "    for i in range(len(toDeleteKeys)):\n",
    "        del ngramDict[toDeleteKeys[i]]\n",
    "        \n",
    "    return ngramDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUnkFreqs(unkNgram):\n",
    "    freqDict = {}\n",
    "    \n",
    "    # creating lists of keys and values\n",
    "    ngramKeyList = list(unkNgram.keys()) \n",
    "    ngramValueList = list(unkNgram.values()) \n",
    "    \n",
    "    for i in range(len(unkNgram)):\n",
    "        currKey = ngramKeyList[i] # getting word/s \n",
    "        currValue = ngramValueList[i] # getting number of appearences\n",
    "\n",
    "        # calculating percentage of frequency\n",
    "        freq = (currValue / (len(words) + len(unkNgram)))\n",
    "\n",
    "        freqDict.update({currKey:freq})\n",
    "        \n",
    "    return freqDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting word/s counts with <UNK> token inserted\n",
    "unkUnigram = unkTokenInsertion(vanillaUnigram)\n",
    "unkBigram = unkTokenInsertion(vanillaBigram)\n",
    "unkTrigram = unkTokenInsertion(vanillaTrigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating unk model percentages\n",
    "unkUnigramFreqs = getUnkFreqs(unkUnigram)\n",
    "unkBigramFreqs = getUnkFreqs(unkBigram)\n",
    "unkTrigramFreqs = getUnkFreqs(unkTrigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding starting (<s>) and closing (</s>) tags to list of words for computation\n",
    "def addSTags(wordList):\n",
    "    wordList.insert(0, startingTag)\n",
    "    wordList.append(closingTag)\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculations related to probability to reduce code redundancy\n",
    "def probabilityCalc(model, wordList, ngramLen, unigramFreq, unigram, bigram, trigram):\n",
    "    res = 1\n",
    "    \n",
    "    if model == 1:\n",
    "        if ngramLen == 1:\n",
    "            freqList = []\n",
    "            \n",
    "            for i in range(len(wordList)):\n",
    "                # checking if all words exist\n",
    "                if unigram[(wordList[i],)] == 0:\n",
    "                    return 0\n",
    "                \n",
    "                # getting freq of every word and storing them in seperate list\n",
    "                freqList.append(unigramFreq[(wordList[i],)])\n",
    "            for i in range(len(freqList)):\n",
    "                res *= freqList[i]\n",
    "\n",
    "            return res\n",
    "        elif ngramLen == 2:\n",
    "            wordList = addSTags(wordList)\n",
    "\n",
    "            for i in range(len(wordList) - 1):\n",
    "                if bigram[(wordList[i], wordList[i+1])] == 0:\n",
    "                    return 0\n",
    "                \n",
    "                if unigram[(wordList[i],)] == 0:\n",
    "                    lastWord = 1\n",
    "                else:\n",
    "                    lastWord = unigram[(wordList[i],)]\n",
    "                prob = bigram[(wordList[i], wordList[i+1])] / lastWord\n",
    "                res *= prob\n",
    "\n",
    "            return res\n",
    "        elif ngramLen == 3:\n",
    "            wordList = addSTags(wordList)\n",
    "            \n",
    "            for i in range(len(wordList) - 2):\n",
    "                if trigram[(wordList[i], wordList[i+1], wordList[i+2])] == 0:\n",
    "                    return 0\n",
    "                # avoiding division by 0 \n",
    "                if bigram[(wordList[i], wordList[i+1])] == 0:\n",
    "                    lastTwoWords = 1\n",
    "                else:\n",
    "                    lastTwoWords = bigram[(wordList[i], wordList[i+1])]\n",
    "                prob = trigram[(wordList[i], wordList[i+1], wordList[i+2])] / lastTwoWords\n",
    "                res *= prob\n",
    "            \n",
    "            return res\n",
    "        else:\n",
    "            print(\"Not valid n-gram count.\")\n",
    "    elif model == 2:        \n",
    "        if ngramLen == 1:           \n",
    "            freqList = []\n",
    "            \n",
    "            for i in range(len(wordList)):\n",
    "                try:\n",
    "                    unigram[wordList[i],]\n",
    "                except Exception:\n",
    "                    return 0\n",
    "                # getting freq of every word and storing them in seperate list\n",
    "                freqList.append(unigramFreq[(wordList[i],)])\n",
    "            for i in range(len(freqList)):\n",
    "                res *= freqList[i]\n",
    "\n",
    "            return res\n",
    "        elif ngramLen == 2:\n",
    "            wordList = addSTags(wordList)\n",
    "\n",
    "            for i in range(len(wordList) - 1):\n",
    "                try:\n",
    "                    bigram[wordList[i], wordList[i+1]]\n",
    "                except Exception:\n",
    "                    return 0\n",
    "                prob = bigram[(wordList[i], wordList[i+1])] / unigram[(wordList[i],)]\n",
    "                res *= prob\n",
    "\n",
    "            return res\n",
    "        elif ngramLen == 3:\n",
    "            wordList = addSTags(wordList)\n",
    "            \n",
    "            for i in range(len(wordList) - 2):\n",
    "                try:\n",
    "                    trigram[wordList[i], wordList[i+1], wordList[i+2]]\n",
    "                except Exception:\n",
    "                    return 0\n",
    "                \n",
    "                if bigram[(wordList[i], wordList[i+1])] == 0:\n",
    "                    lastTwoWords = 1\n",
    "                else:\n",
    "                    lastTwoWords = bigram[(wordList[i], wordList[i+1])]\n",
    "                    prob = trigram[(wordList[i], wordList[i+1], wordList[i+2])] / lastTwoWords\n",
    "                    res *= prob\n",
    "            \n",
    "            return res\n",
    "        else:\n",
    "            print(\"Not valid n-gram count.\")\n",
    "    # extra bits for unk calculations                                   \n",
    "    elif model == 3:\n",
    "        if ngramLen == 1:\n",
    "            freqList = []\n",
    "            # getting freq of every word and storing them in seperate list\n",
    "            for i in range(len(wordList)):\n",
    "                try:\n",
    "                    currWordFreq = unigramFreq[(wordList[i],)]\n",
    "                except Exception:\n",
    "                    currWordFreq = unigramFreq[unkString]\n",
    "                freqList.append(currWordFreq)\n",
    "                \n",
    "            for i in range(len(freqList)):\n",
    "                res *= freqList[i]\n",
    "\n",
    "            return res\n",
    "        elif ngramLen == 2:\n",
    "            wordList = addSTags(wordList)\n",
    "             \n",
    "            for i in range(len(wordList) - 1):\n",
    "                # if phrases are not in vocab then replace count by UNK tag\n",
    "                try:\n",
    "                    allPhrase = bigram[(wordList[i], wordList[i+1])]\n",
    "                except Exception:\n",
    "                    allPhrase = bigram[unkString]\n",
    "                try:\n",
    "                    firstWord = unigram[(wordList[i],)]\n",
    "                except Exception:\n",
    "                    firstWord = unigram[unkString]\n",
    "                \n",
    "                prob = firstWord / allPhrase\n",
    "                res *= prob\n",
    "                \n",
    "            return res\n",
    "        elif ngramLen == 3:\n",
    "            wordList = addSTags(wordList)\n",
    "             \n",
    "            for i in range(len(wordList) - 2):\n",
    "                # if phrases are not in vocab then replace count by UNK tag\n",
    "                try:\n",
    "                    allPhrase = trigram[(wordList[i], wordList[i+1], wordList[i+2])]\n",
    "                except Exception:\n",
    "                    allPhrase = trigram[unkString]\n",
    "                try:\n",
    "                    firstTwoWords = bigram[(wordList[i], wordList[i+1])]\n",
    "                except Exception:\n",
    "                    firstTwoWords = bigram[unkString]\n",
    "                \n",
    "                prob = firstTwoWords / allPhrase \n",
    "                res *= prob\n",
    "                \n",
    "            return res\n",
    "        else:\n",
    "            print(\"Not valid n-gram count.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper method to find probabilty of phrase (for model: 1 - Vanilla, 2 - Laplace, 3 - UNK)\n",
    "def probability(string, model, ngramLen):\n",
    "    # splitting the string from the spaces to get a list with the individual words\n",
    "    string = string.lower()\n",
    "    wordList = re.findall(r\"[\\w']+|[.,!?;]\", string)\n",
    "    \n",
    "    if model == 1:\n",
    "        return probabilityCalc(model, wordList, ngramLen, vanillaUnigramFreqs, vanillaUnigram, vanillaBigram, vanillaTrigram)\n",
    "    elif model == 2:\n",
    "        return probabilityCalc(model, wordList, ngramLen, laplaceUnigramFreqs, laplaceUnigram, laplaceBigram, laplaceTrigram)\n",
    "    elif model == 3:\n",
    "        return probabilityCalc(model, wordList, ngramLen, unkUnigramFreqs, unkUnigram, unkBigram, unkTrigram)\n",
    "    else:\n",
    "        print(\"Not valid model choice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard coded weights for ngrams\n",
    "unigramLamba = 0.1\n",
    "bigramLamba = 0.3\n",
    "trigramLamba = 0.6\n",
    "\n",
    "# linear interpolation method (for model: 1 - Vanilla, 2 - Laplace, 3 - UNK)\n",
    "def li(string, model):\n",
    "    unigramWeight = probability(string, model, 1) * unigramLamba\n",
    "    bigramWeight = probability(string, model, 2) * bigramLamba\n",
    "    trigramWeight = probability(string, model, 3) * trigramLamba\n",
    "    \n",
    "    return unigramWeight + bigramWeight + trigramWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get perplexity of a sentence (li - boolean if linear interpolation should be used)\n",
    "def perplexityForString(string, model, ngram, isLi):\n",
    "    n = len(re.findall(r\"[\\w']+|[.,!?;]\", string))\n",
    "    if isLi == True:    \n",
    "        prob = li(string, model)\n",
    "        if prob == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return pow(prob, -1/n)\n",
    "    else:\n",
    "        prob = probability(string, model, ngram)\n",
    "        if prob == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return pow(prob, -1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9833589960533025\n"
     ]
    }
   ],
   "source": [
    "print(perplexityForString(\"This is one\", 3, 3, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexityForFiles(folderName, model, ngram, isLi):\n",
    "    words = []\n",
    "    words = parseXml(folderName, words)\n",
    "\n",
    "    # getting sentences from words in tokenised form\n",
    "    formattedWords = tempSentence = []\n",
    "    counter = 0\n",
    "    for i in range(len(words)):\n",
    "        # if start of sentence then continue\n",
    "        if words[i] == '<s>':\n",
    "            continue\n",
    "        elif words[i] == '</s>': \n",
    "            # if end of sentence then add completed sentence to list\n",
    "            formattedWords.insert(counter, tempSentence)\n",
    "            counter += 1\n",
    "            tempSentence = []                   \n",
    "        else:\n",
    "            tempSentence.append(words[i])\n",
    "    \n",
    "    # first element and last 23 was being false info\n",
    "    del formattedWords[0]\n",
    "    del formattedWords[-23:]\n",
    "    \n",
    "    # taking each sentence and making it 1 string to then pass through other methods\n",
    "    tempString = \"\"\n",
    "    formattedSentences = []\n",
    "    for sentence in formattedWords:\n",
    "        for word in sentence:\n",
    "            tempString += \" \" + word\n",
    "        formattedSentences.append(tempString)\n",
    "        tempString = \"\"\n",
    "    \n",
    "    if isLi == True:  \n",
    "        totalProb = tempProb = 0 \n",
    "        for i in range(len(formattedSentences)):\n",
    "            tempProb = li(formattedSentences[i], model)\n",
    "            totalProb += tempProb\n",
    "        averageProb = totalProb / len(formattedSentences)\n",
    "        \n",
    "        if averageProb == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return pow(averageProb, -1/len(words)) \n",
    "    else:\n",
    "        totalProb = tempProb = 0 \n",
    "        for i in range(len(formattedSentences)):\n",
    "            tempProb = probability(formattedSentences[i], model, ngram)\n",
    "            totalProb += tempProb\n",
    "        averageProb = totalProb / len(formattedSentences)\n",
    "\n",
    "        if averageProb == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return pow(averageProb, -1/len(words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998134694600608\n"
     ]
    }
   ],
   "source": [
    "print(perplexityForFiles('\\\\testing_set\\\\', 3, 3, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textGenWrapper(string, model):\n",
    "    if model == 1:\n",
    "        return textGeneration(string, vanillaTrigramFreqs)\n",
    "    elif model == 2:\n",
    "        return textGeneration(string, laplaceTrigramFreqs)\n",
    "    elif model == 3:\n",
    "        return textGeneration(string, unkTrigramFreqs)\n",
    "    else:\n",
    "        print(\"Not a valid model selection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textGeneration(string, model):\n",
    "    finalString = string\n",
    "    tokenisedString = re.findall(r\"[\\w']+|[.,!?;]\", string)\n",
    "    \n",
    "    modelList = list(model.keys())\n",
    "    \n",
    "    lastTwo = tokenisedString[-2:]\n",
    "    \n",
    "    # getting tuples that include the last 2 words of inputted string\n",
    "    includesTuple = [item for item in model if item[0] == lastTwo[0] and item[1] == lastTwo[1]]\n",
    "    \n",
    "    maxIndex = maxFreq = 0\n",
    "    maxTuple = ()\n",
    "    for i in range(len(includesTuple)):\n",
    "        if model[includesTuple[i]] > maxFreq:\n",
    "            maxFreq = model[includesTuple[i]]\n",
    "            maxIndex = i\n",
    "            maxTuple = includesTuple[i]\n",
    "    \n",
    "    # if there are no possible continuations just return\n",
    "    if maxTuple == ():\n",
    "        return finalString\n",
    "    \n",
    "    finalString += \" \" + maxTuple[2]\n",
    "    lastTwo[0] = lastTwo[1]\n",
    "    lastTwo[1] = maxTuple[2]\n",
    "    \n",
    "    while lastTwo[1] != '</s>':\n",
    "        includesTuple = [item for item in model if item[0] == lastTwo[0] and item[1] == lastTwo[1]]\n",
    "        \n",
    "        maxIndex = maxFreq = 0\n",
    "        maxTuple = ()\n",
    "        for i in range(len(includesTuple)):\n",
    "            if model[includesTuple[i]] > maxFreq:\n",
    "                maxFreq = model[includesTuple[i]]\n",
    "                maxIndex = i\n",
    "                maxTuple = includesTuple[i]\n",
    "        \n",
    "        # if there are no possible continuations just return\n",
    "        if maxTuple == ():\n",
    "            return finalString\n",
    "        \n",
    "        finalString += \" \" + maxTuple[2]\n",
    "        lastTwo[0] = lastTwo[1]\n",
    "        lastTwo[1] = maxTuple[2]\n",
    "    \n",
    "    # removes the ending string tag\n",
    "    return finalString[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"only recently we have n't got a lot of people who are you ? \""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textGenWrapper(\"only recently we have\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportModelToJson(model, filename):\n",
    "    newDict = {}\n",
    "    \n",
    "    # have to convert keys from tuples to strings to be exported\n",
    "    keyList = list(model.keys())\n",
    "    valueList = list(model.values())\n",
    "    \n",
    "    for i in range(len(keyList)):\n",
    "        newDict.update({str(keyList[i]):valueList[i]})\n",
    "    \n",
    "    with open(filename + '.json', 'w') as fp:\n",
    "        json.dump(newDict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting all models to JSON\n",
    "exportModelToJson(vanillaUnigramFreqs, \"vanillaUnigramModel\")\n",
    "exportModelToJson(vanillaBigramFreqs, \"vanillaBigramModel\")\n",
    "exportModelToJson(vanillaTrigramFreqs, \"vanillaTrigramModel\")\n",
    "\n",
    "exportModelToJson(laplaceUnigramFreqs, \"laplaceUnigramModel\")\n",
    "exportModelToJson(laplaceBigramFreqs, \"laplaceBigramModel\")\n",
    "exportModelToJson(laplaceTrigramFreqs, \"laplaceTrigramModel\")\n",
    "\n",
    "exportModelToJson(unkUnigramFreqs, \"unkUnigamModel\")\n",
    "exportModelToJson(unkBigramFreqs, \"unkBigramModel\")\n",
    "exportModelToJson(unkTrigramFreqs, \"unkTrigramModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
